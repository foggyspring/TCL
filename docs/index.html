<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="imitation learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data </title>
  <link rel="icon" type="image/x-icon" href="static/images/capybara.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://hk-zh.github.io/Homepage/" target="_blank">Hongkuan Zhou</a>,
              </span>
              <span class="author-block">
                <a href="https://www.ce.cit.tum.de/air/people/zhenshan-bing-drrernat/" target="_blank">Zhenshan Bing</a><sup>&#8224;</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.ce.cit.tum.de/air/people/xiangtong-yao/" target="_blank">Xiangtong Yao</a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Xiaojie Su</a>
              </span>
              <span class="author-block">
                <a href="" target="_blank">Chenguang Yang</a>
              </span>
              <span class="author-block">
                <a href="" target="_blank">Kai Huang</a>
              </span>
              <span class="author-block">
                <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/" target="_blank">Alios Knoll</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Technical University of Munich</span>
              <span class="eql-cntrb"><small><br> <sup>&#8224;</sup>Corresponding Author</small></span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2305.19075.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                        </a>
                      </span>

                    <!-- Supplementary PDF link -->
<!--                      <span class="link-block">-->
<!--                        <a href="static/pdfs/supplementary_material.pdf" target="_blank"-->
<!--                        class="external-link button is-normal is-rounded is-dark">-->
<!--                        <span class="icon">-->
<!--                          <i class="fas fa-file-pdf"></i>-->
<!--                        </span>-->
<!--                        <span>Supplementary</span>-->
<!--                        </a>-->
<!--                      </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/hk-zh/spil" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2305.19075" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/Single%20Task%20Completion.mp4"
        type="video/mp4">
      </video>

    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The growing interest in language-conditioned robot manipulation aims to develop robots capable of understanding and executing complex tasks, with the objective of enabling robots to interpret language commands and manipulate objects accordingly. While language-conditioned approaches demonstrate impressive capabilities for addressing tasks in familiar environments, they encounter limitations in adapting to unfamiliar environment settings. In this study, we propose a general-purpose, language-conditioned approach that combines base skill priors and imitation learning under unstructured data to enhance the algorithm's generalization in adapting to unfamiliar environments. We assess our model's performance in both simulated and real-world environments using a zero-shot setting. In the simulated environment, the proposed approach surpasses previously reported scores for CALVIN benchmark, especially in the challenging Zero-Shot Multi-Environment setting. The average completed task length, indicating the average number of tasks the agent can continuously complete, improves more than 2.5 times compared to the state-of-the-art method HULC. In addition, we conduct a zero-shot evaluation of our policy in a real-world setting, following training exclusively in simulated environments without additional specific adaptations. In this evaluation, we set up ten tasks and achieved an average 30\% improvement in our approach compared to the current state-of-the-art approach, demonstrating a high generalization capability in both simulated environments and the real world.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <div class="carousel results-carousel">

        <div class="item">
          <!-- Your image here -->
          <img src="static/images/skill_space.png" alt="MY ALT TEXT"/>
          <div class="content has-text-justified">
            <p>
              This architecture comprises two encoders - the skill prior encoder and the action sequence encoder, and a decoder for reconstructing the skill embeddings into action sequences. The skill prior encoder takes one-hot-key embeddings of translation, rotation, and grasping as input and outputs the distribution of the base skill prior in the skill latent space. The action sequence encoder encodes the action sequences with a fixed horizon of &hamilt; to the distribution of skill in the latent space. The decoder then reconstructs the skill embedding into action sequences.
            </p>
          </div>
        </div>
        <div class="item">
          <!-- Your image here -->
          <figure class="image">
            <img src="static/images/architecture.png" alt="MY ALT TEXT"/> <br>
            <div class="content has-text-justified">
              <p>
                  The architecture of the proposed method. Following the encoding process, the static observation, gripper observation, and language instruction are generated to embeddings for the plan, language goal, language, static observation, and gripper observation. The skill selector module subsequently decodes a sequence of skill embeddings using the plan, observation, and language goal embeddings. The skill labeler is responsible for labelling the skill embeddings with the base skills:  translation, rotation, and grasping. The base skill regularization loss is calculated based on the base skill prior distributions (from base skill locator &#8492;), selected skill instance, and labelled probability indicating its belonging to specific base skills. This labelled probability is also leveraged to determine the categorical regularization loss. Finally, the pre-trained and frozen skill generator &Gscr; decodes all the skill embeddings into action sequences, which are then utilized to calculate the reconstruction loss (Huber loss).
              </p>
            </div>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--      <h2 class="title is-3">Video Presentation</h2>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--        <div class="column is-four-fifths">-->
<!--          -->
<!--          <div class="publication-video">-->
<!--            &lt;!&ndash; Youtube embed code here &ndash;&gt;-->
<!--            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!-- End youtube video -->


<!-- Video carousel -->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container is-max-desktop content">-->
<!--      <h2 class="title is-3">Videos</h2>-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item item-video1">-->
<!--          <video poster="" id="video1" autoplay controls muted loop height="100%">-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/videoplayback.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-video2">-->
<!--          <video poster="" id="video2" autoplay controls muted loop height="100%">-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/Sequential%20Task%20Completion.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!-- End video carousel -->

  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <h2 class="title is-3">Videos</h2>
        <div class="columns is-vcentered">
          <div class="column is-5">
            <div class="column is-centered has-text-centered">
              <video poster="" id="video1" width="300" height="300" controls>
                <!-- Your video file here -->
                <source src="static/videos/videoplayback.mp4"
                type="video/mp4">
              </video>
            </div>

          </div>
          <div class="column">
            <div class="column is-centered has-text-centered">
              <video poster="" id="video2" width="534" height="300" controls>
                <!-- Your video file here -->
                <source src="static/videos/Sequential%20Task%20Completion.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
        <div>
    </div>
  </section>




<!-- Paper poster -->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title">Poster</h2>-->

<!--      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">-->
<!--          </iframe>-->
<!--        -->
<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{zhou2023languageconditioned,
              title={Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data},
              author={Hongkuan Zhou and Zhenshan Bing and Xiangtong Yao and
              Xiaojie Su and Chenguang Yang and Kai Huang and Alois Knoll},
              year={2023},
              eprint={2305.19075},
              archivePrefix={arXiv},
              primaryClass={cs.RO}
        }
      </code></pre>
    </div>
  </section>
<!--End BibTex citation -->

  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <div class="columns is-vcentered">
          <div class="column">
            <div class="column is-centered has-text-centered">
              <span class="icon">
                <i class="fa fa-map-marker"></i>
              </span>
              <div class="content">
                <h4>Address</h4>
                <p>Boltzmannstr. 3, 85748 Garching, Germany</p>
              </div>
            </div>

          </div>
          <div class="column">
            <div class="column is-centered has-text-centered">
              <span class="icon">
                <i class="fas fa-envelope"></i>
              </span>
              <div class="content">
                <h4>Contact</h4>
                <a href="mailto:zhenshan.bing@tum.de">zhenshan.bing@tum.de</a>
              </div>
            </div>
          </div>
        </div>
        <div>
    </div>
  </section>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This work is supported by Lehrstuhl für Robotik, Künstliche Intelligenz und Echtzeitsysteme TUM School of Computation, Information and Technology Technische Universität München.
          </p>

        </div>
      </div>
    </div>
  </div>
  </footer>


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
